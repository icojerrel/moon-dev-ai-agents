# ğŸ”§ Docker + Ollama Architectuur: Hoe Het Samenwerkt

## ğŸ“Š High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Docker Host (jouw machine)              â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚        Docker Network: moondev-network             â”‚    â”‚
â”‚  â”‚                                                    â”‚    â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚   â”‚   Ollama     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Trading     â”‚    â”‚    â”‚
â”‚  â”‚   â”‚  Container   â”‚  HTTP API  â”‚   Agents      â”‚    â”‚    â”‚
â”‚  â”‚   â”‚              â”‚            â”‚  Container    â”‚    â”‚    â”‚
â”‚  â”‚   â”‚ qwen3-coder  â”‚            â”‚               â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   :30b       â”‚            â”‚ ModelFactory  â”‚    â”‚    â”‚
â”‚  â”‚   â”‚              â”‚            â”‚    â†“          â”‚    â”‚    â”‚
â”‚  â”‚   â”‚ Port: 11434  â”‚            â”‚  RBI Agent    â”‚    â”‚    â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚         â†“                             â†“            â”‚    â”‚
â”‚  â”‚   [ollama_data]               [./src, ./data]     â”‚    â”‚
â”‚  â”‚    (persistent)                  (bind mounts)    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                             â”‚
â”‚  Host Ports:                                                â”‚
â”‚  â”œâ”€ localhost:11434 â†’ ollama:11434                          â”‚
â”‚  â””â”€ Direct container network (faster!)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Communication Flow

### Stap-voor-stap: Wat er gebeurt bij een RBI Agent request

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. USER STARTS RBI AGENT                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ docker-compose --profile rbi up rbi-agent                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. RBI CONTAINER STARTS                                       â”‚
â”‚    â”œâ”€ Loads src/ from bind mount                             â”‚
â”‚    â”œâ”€ Reads ideas.txt                                         â”‚
â”‚    â””â”€ Initializes ModelFactory                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. MODELFACTORY INITIALIZATION                                â”‚
â”‚                                                               â”‚
â”‚    from src.models.model_factory import ModelFactory         â”‚
â”‚    factory = ModelFactory()                                  â”‚
â”‚    model = factory.get_model("ollama", "qwen3-coder:30b")    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. HEALTH CHECK                                               â”‚
â”‚                                                               â”‚
â”‚    requests.get("http://ollama:11434/api/tags")              â”‚
â”‚    â†“                                                          â”‚
â”‚    [Docker DNS resolves "ollama" to Ollama container IP]     â”‚
â”‚    â†“                                                          â”‚
â”‚    Response: {"models": [{"name": "qwen3-coder:30b"}]}       â”‚
â”‚    âœ… Ollama is ready!                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. RESEARCH PHASE                                             â”‚
â”‚                                                               â”‚
â”‚    RBI Agent:                                                 â”‚
â”‚    â”œâ”€ Reads trading idea: "RSI Divergence Strategy"          â”‚
â”‚    â””â”€ Calls: model.generate_response(                        â”‚
â”‚              system_prompt="You are a research AI...",       â”‚
â”‚              user_content="Analyze this strategy...")        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. HTTP REQUEST TO OLLAMA                                     â”‚
â”‚                                                               â”‚
â”‚    POST http://ollama:11434/api/generate                     â”‚
â”‚    {                                                          â”‚
â”‚      "model": "qwen3-coder:30b",                              â”‚
â”‚      "prompt": "System: You are...\n\nUser: Analyze...",     â”‚
â”‚      "stream": false,                                         â”‚
â”‚      "options": {                                             â”‚
â”‚        "temperature": 0.7,                                    â”‚
â”‚        "num_predict": 2048                                    â”‚
â”‚      }                                                         â”‚
â”‚    }                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. OLLAMA PROCESSES REQUEST                                   â”‚
â”‚                                                               â”‚
â”‚    Ollama Container:                                          â”‚
â”‚    â”œâ”€ Loads qwen3-coder:30b from /root/.ollama/models        â”‚
â”‚    â”œâ”€ Runs inference (GPU/CPU)                               â”‚
â”‚    â”œâ”€ Time: ~10 seconds for 500 tokens                       â”‚
â”‚    â””â”€ Returns JSON response                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. RESPONSE FLOWS BACK                                        â”‚
â”‚                                                               â”‚
â”‚    {                                                          â”‚
â”‚      "response": "STRATEGY_NAME: RSIDivergence\n\n...",       â”‚
â”‚      "done": true,                                            â”‚
â”‚      "total_duration": 8234567890,                            â”‚
â”‚      "load_duration": 123456789                               â”‚
â”‚    }                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. RBI AGENT PROCESSES RESPONSE                               â”‚
â”‚                                                               â”‚
â”‚    â”œâ”€ Extracts strategy name: "RSIDivergence"                â”‚
â”‚    â”œâ”€ Saves to: src/data/rbi/01_15_2025/research/            â”‚
â”‚    â””â”€ Proceeds to BACKTEST PHASE                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10. REPEAT FOR EACH PHASE                                     â”‚
â”‚                                                               â”‚
â”‚     Research  âœ… â†’ qwen3-coder (12s)                          â”‚
â”‚     Backtest  âœ… â†’ qwen3-coder (18s)                          â”‚
â”‚     Package   âœ… â†’ qwen3-coder (9s)                           â”‚
â”‚     Debug     âœ… â†’ qwen3-coder (8s)                           â”‚
â”‚                                                               â”‚
â”‚     Total: ~47 seconds, $0.00 cost                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Docker Compose Configuration Breakdown

```yaml
# docker-compose.yml

services:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # OLLAMA SERVICE - The AI Brain
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  ollama:
    image: ollama/ollama:latest
    container_name: moondev-ollama

    # Port mapping: host:container
    # Agents connect via "http://ollama:11434" (internal)
    # You can test via "http://localhost:11434" (external)
    ports:
      - "11434:11434"

    # Persistent storage for models (15GB for qwen3-coder:30b)
    # Without this, you'd re-download model on every restart!
    volumes:
      - ollama_data:/root/.ollama

    # Join the moondev network so agents can communicate
    networks:
      - moondev-network

    # Auto-restart if it crashes
    restart: unless-stopped

    # Environment: Listen on all interfaces (not just localhost)
    environment:
      - OLLAMA_HOST=0.0.0.0

    # Health check: Ensures Ollama is ready before starting agents
    # Agents depend on this check passing!
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s      # Check every 30 seconds
      timeout: 10s       # Fail if takes > 10 seconds
      retries: 3         # Try 3 times before marking unhealthy
      start_period: 60s  # Wait 60s after start before checking

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # RBI AGENT SERVICE - Strategy Generator
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  rbi-agent:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: moondev-rbi

    # Don't start until Ollama is HEALTHY (not just running)
    depends_on:
      ollama:
        condition: service_healthy

    # Bind mounts: Live code sync
    # Changes to ./src immediately visible in container!
    volumes:
      - ./src:/app/src        # Code
      - ./data:/app/data      # Persistent data
      - ./.env:/app/.env:ro   # Environment (read-only)

    # Join network to communicate with Ollama
    networks:
      - moondev-network

    # Environment variables
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434  # Internal DNS
      - PYTHONUNBUFFERED=1                    # See logs in real-time

    # Only start with: docker-compose --profile rbi up
    profiles:
      - rbi

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SHARED NETWORK - Internal DNS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
networks:
  moondev-network:
    driver: bridge    # Default Docker network driver
    # Automatically provides DNS:
    # - "ollama" resolves to Ollama container IP
    # - "rbi-agent" resolves to RBI container IP

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PERSISTENT STORAGE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
volumes:
  ollama_data:
    driver: local
    # Stores models at: /var/lib/docker/volumes/ollama_data
    # Survives container restarts and rebuilds
```

---

## ğŸ”Œ Network Communication Details

### Internal Communication (Container â†’ Container)

```python
# In RBI Agent container:
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import requests

# This hostname "ollama" is resolved by Docker DNS
# to the Ollama container's internal IP (e.g., 172.18.0.2)
response = requests.post(
    "http://ollama:11434/api/generate",
    json={
        "model": "qwen3-coder:30b",
        "prompt": "Write a Python function...",
        "stream": False
    }
)

# Docker network routing:
# rbi-agent (172.18.0.3) â†’ ollama (172.18.0.2)
# Fast: No network interface, just internal routing
# Latency: <1ms
```

### External Testing (Host â†’ Container)

```bash
# From your laptop/desktop:
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Test Ollama is responding
curl http://localhost:11434/api/tags

# Generate text
curl http://localhost:11434/api/generate -d '{
  "model": "qwen3-coder:30b",
  "prompt": "Hello!",
  "stream": false
}'

# Docker port mapping:
# localhost:11434 (host) â†’ 11434 (container)
```

---

## ğŸ“‚ File System: How Data Flows

### Bind Mounts (Live Sync)

```
Host Machine                    RBI Container
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
./src/                    â†’    /app/src/
â”œâ”€â”€ agents/               â†’    â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ rbi_agent.py      â†’    â”‚   â”œâ”€â”€ rbi_agent.py    # Same file!
â”‚   â””â”€â”€ chat_agent.py     â†’    â”‚   â””â”€â”€ chat_agent.py
â”œâ”€â”€ models/               â†’    â”œâ”€â”€ models/
â”‚   â””â”€â”€ model_factory.py  â†’    â”‚   â””â”€â”€ model_factory.py
â””â”€â”€ config.py             â†’    â””â”€â”€ config.py

./data/                   â†’    /app/data/
â””â”€â”€ rbi/                  â†’    â””â”€â”€ rbi/
    â”œâ”€â”€ ideas.txt         â†’        â”œâ”€â”€ ideas.txt
    â””â”€â”€ 01_15_2025/       â†’        â””â”€â”€ 01_15_2025/
        â”œâ”€â”€ research/     â†’            â”œâ”€â”€ research/
        â””â”€â”€ backtests/    â†’            â””â”€â”€ backtests/
```

**Key Point:** Edit `src/agents/rbi_agent.py` on your laptop â†’ instantly available in container!

### Volume (Persistent Storage)

```
Ollama Container              Docker Volume
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
/root/.ollama/         â†’     /var/lib/docker/volumes/
â”œâ”€â”€ models/            â†’         moondev_ollama_data/_data/
â”‚   â””â”€â”€ blobs/         â†’             models/
â”‚       â””â”€â”€ sha256-... â†’                 blobs/
â”‚           (15GB)     â†’                     sha256-abc123...
â”‚                      â†’                     (qwen3-coder:30b)
â””â”€â”€ manifests/         â†’             manifests/
```

**Key Point:** Model persists across restarts. No re-download needed!

---

## âš¡ Performance: Why It's Fast

### 1. **No Network Overhead**

```
Traditional API Call:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Your Code â†’ Internet â†’ OpenAI Data Center â†’ Internet â†’ Your Code
Latency:  10ms        50-200ms              50-200ms   10ms
Total: ~120-420ms + processing time

Docker Internal:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RBI Container â†’ Docker Network â†’ Ollama Container
Latency:      0.1ms               0.1ms
Total: ~0.2ms + processing time

ğŸš€ Network overhead: 0.2ms vs 120-420ms (600-2000x faster!)
```

### 2. **Model Always Loaded**

```
Cloud API:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Your Request â†’ Queue â†’ Load Model â†’ Inference â†’ Return
Time:         varies   2-5s         8s          varies

Ollama Docker:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Your Request â†’ Inference (model pre-loaded) â†’ Return
Time:         0ms       8s                      0ms

ğŸš€ No queue, no cold start
```

### 3. **Parallel Processing**

```yaml
# Scale horizontally with one command:
docker-compose --profile rbi up --scale rbi-agent=5

# Now you have:
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚   Ollama    â”‚â—„â”€â”€ rbi-agent-1
# â”‚  Container  â”‚â—„â”€â”€ rbi-agent-2
# â”‚ (shared)    â”‚â—„â”€â”€ rbi-agent-3
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â—„â”€â”€ rbi-agent-4
#               â—„â”€â”€ rbi-agent-5

# Process 5 strategies simultaneously!
# Cost: Still $0.00
```

---

## ğŸ”§ Practical Examples

### Example 1: Test Ollama from Host

```bash
# List loaded models
curl http://localhost:11434/api/tags

# Output:
# {
#   "models": [
#     {
#       "name": "qwen3-coder:30b",
#       "modified_at": "2025-01-15T10:30:00Z",
#       "size": 16894828517
#     }
#   ]
# }
```

### Example 2: Agent Connects to Ollama

```python
# src/models/ollama_model.py (simplified)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import requests
import os

class OllamaModel:
    def __init__(self, model_name="qwen3-coder:30b"):
        # Get base URL from environment or use default
        self.base_url = os.getenv(
            "OLLAMA_BASE_URL",
            "http://ollama:11434"  # Docker internal DNS
        )
        self.model_name = model_name

    def generate_response(self, system_prompt, user_content, temperature=0.7):
        url = f"{self.base_url}/api/generate"

        payload = {
            "model": self.model_name,
            "prompt": f"System: {system_prompt}\n\nUser: {user_content}",
            "stream": False,
            "options": {
                "temperature": temperature,
            }
        }

        # Docker network resolves "ollama" to container IP
        response = requests.post(url, json=payload)
        return response.json()["response"]
```

### Example 3: Debug Connection Issues

```bash
# From inside RBI container:
docker-compose exec rbi-agent bash

# Test Ollama connectivity
curl http://ollama:11434/api/tags
# âœ… Works: Docker DNS resolves "ollama"

curl http://localhost:11434/api/tags
# âŒ Fails: localhost = rbi container, not Ollama

# Check network
docker network inspect moondev-network
# Shows both containers on same network with IPs
```

---

## ğŸ¯ Why This Architecture Is "Geolied"

### 1. **Service Isolation**
- Ollama crashes? Agents keep running (restart policy handles it)
- Update agents? Ollama unaffected (bind mounts, no rebuild)

### 2. **Zero Configuration**
- Agents automatically discover Ollama via DNS
- No IP addresses to configure
- No port conflicts

### 3. **Reproducible**
```bash
# Same setup on any machine:
git clone repo
docker-compose up -d
# Done!
```

### 4. **Scalable**
```bash
# Need more power?
docker-compose up --scale rbi-agent=10
# 10x parallelism, same cost
```

### 5. **Observable**
```bash
# Monitor in real-time
docker-compose logs -f ollama     # See AI processing
docker-compose logs -f rbi-agent  # See agent output
docker stats                      # Resource usage
```

---

## ğŸ”¥ The "Sublime Samenwerking"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ What Makes It Sublime:                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ âœ… Health Checks         â†’ Agents wait for Ollama      â”‚
â”‚ âœ… DNS Resolution        â†’ No hardcoded IPs            â”‚
â”‚ âœ… Bind Mounts           â†’ Live code sync              â”‚
â”‚ âœ… Persistent Volumes    â†’ Models survive restarts     â”‚
â”‚ âœ… Internal Network      â†’ <1ms latency               â”‚
â”‚ âœ… Automatic Restart     â†’ Self-healing system        â”‚
â”‚ âœ… Profile-based Start   â†’ Start only what you need   â”‚
â”‚ âœ… Environment Isolation â†’ No dependency conflicts    â”‚
â”‚                                                         â”‚
â”‚ Result: A machine that just worksâ„¢                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ TL;DR

1. **Ollama container** hosts qwen3-coder:30b (15GB model)
2. **RBI agent container** runs your trading agents
3. **Docker network** provides internal DNS ("ollama" hostname)
4. **Communication** happens via HTTP API (port 11434)
5. **Latency** is <1ms (internal network, no internet)
6. **Cost** is $0 (all local)
7. **Data** persists via volumes (models) and bind mounts (code)
8. **Scaling** is trivial (docker-compose up --scale)

**This is why it's a geolied machine die subliem samenwerkt.** ğŸŒ™
